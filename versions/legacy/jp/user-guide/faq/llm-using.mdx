---
title: LLM の設定と使用
version: '日本語'
---

### **1. OpenAIプロキシサーバーの利用方法は？**

DifyはOpenAIのカスタムAPIドメイン機能をサポートしており、OpenAI互換の大規模モデルAPIサーバーにも対応しています。

### **2. ベースモデルの選択方法は？**

* gpt-3.5-turbo: gpt-3.5-turboはgpt-3モデルシリーズのアップグレード版で、gpt-3よりも強力で、より複雑なタスクを処理できます。特に長文やクロスドキュメントの推論能力が大幅に向上しています。gpt-3.5-turboは一貫性のある説得力のあるテキストを生成し、要約や翻訳、創造的な執筆の分野でも大きな進展を見せています。得意な分野: 長文理解、クロスドキュメント推論、要約、翻訳、創造的執筆。
* gpt-4: gpt-4は最新かつ最も強力なTransformer言語モデルです。事前トレーニングされたパラメータの数は約200億に増加しており、全ての言語タスクで最高水準の性能を誇ります。特に、長く複雑な応答を深く理解し生成するタスクにおいて優れた能力を発揮します。gpt-4は、人間の言語のあらゆる側面を扱うことができ、抽象的な概念の理解やクロスページの推論も可能です。gpt-4は実質的に初の本当の汎用言語理解システムであり、人工知能分野におけるすべての自然言語処理タスクに対応できます。得意な分野: すべてのNLPタスク、言語理解、長文生成、クロスドキュメント推論、抽象概念理解。 詳細はドキュメントをご覧ください。

### **3. max_tokensを小さく設定する理由は？**

自然言語処理において、出力するテキストが長くなるほど、計算にかかる時間とリソースが増加します。したがって、出力テキストの長さを制限することで、計算コストや時間を抑えることができます。例えば、max_tokensを500に設定すると、出力テキストは最初の500トークンまでしか考慮されず、それ以上の部分は無視されます。これにより、出力テキストの長さがモデルの受け入れ範囲を超えないようにしつつ、計算リソースを効率的に活用してモデルの実行効率を高めることができます。また、max_tokensを制限することでプロンプトに使用できるトークン数が増えます。例えば、gpt-3.5-turboの制限は4097トークンで、max_tokensを4000に設定すると、プロンプトには97トークンしか使えません。それを超えるとエラーが発生します。

### **4. データセットの長いテキストを適切に分割する方法は？**

多くの自然言語処理アプリケーションでは、通常、文や段落ごとにテキストを分割して、意味情報や構造情報をより効果的に処理・理解します。最小の分割単位は具体的なタスクや技術の実装によって異なります。例えば：

* テキスト分類タスクでは、文や段落ごとにテキストを分割します。
* 機械翻訳タスクでは、完全な文や段落を分割単位として使用することが求められます。

最適な埋め込み技術と分割単位を決定するためには、実験と評価が必要です。異なる技術や分割単位の性能をテストセットで比較し、最適な解決策を選ぶことができます。

### 5. データセットのセグメンテーションに使用される距離関数は何ですか？

私たちは[コサイン類似度](https://en.wikipedia.org/wiki/Cosine_similarity)を使用しています。通常、距離関数の選択はそれほど重要ではありません。OpenAIの埋め込みは長さ1に正規化されているため、次のようになります：

コサイン類似度を計算する際には内積のみを使用することで、わずかに計算時間を短縮できます。

コサイン類似度とユークリッド距離は同じランキングを生成します。

* 正規化された埋め込みベクトルを用いてコサイン類似度またはユークリッド距離を計算し、その類似度に基づいてベクトルをソートすると、結果は同じになります。つまり、ベクトル間の類似性を測定するためにコサイン類似度またはユークリッド距離を使用した場合、ソートされた結果は一致します。これは、正規化された後ではベクトルの長さが相対関係に影響を与えなくなり、方向情報のみが保持されるためです。したがって、正規化されたベクトルを使用して類似性を測定する場合、異なる尺度を用いても同じソート結果が得られます。ベクトルを正規化することで、すべてのベクトルの長さが1にスケーリングされ、すべてが単位長さ上に位置することを意味します。単位ベクトルはサイズを持たず、方向のみを表します。_詳細はChatGPTにお問い合わせください。_

埋め込みベクトルが長さ1に正規化されている場合、2つのベクトル間のコサイン類似度の計算は、それらの内積の計算に簡略化されます。正規化されたベクトルの長さが全て1であるため、内積の結果はコサイン類似度の結果と等しくなります。内積演算は他の類似性尺度（例：ユークリッド距離）に比べて計算速度が速いため、正規化されたベクトルを用いて内積を計算することで、計算効率が若干向上します。

### 6. OpenAIのAPIキーを入力すると、「**校验失败： You exceeded your current quota， please check your plan and billing details。**」というエラーメッセージが表示される理由は何ですか？

このエラーメッセージは、OpenAIキーのアカウントに料金が支払われていないことを示しています。OpenAIにアクセスして料金を追加してください。

### 7. OpenAIのAPIキーを使用してアプリ内で対話を行う際に、以下のエラーメッセージが表示される場合、その原因は何ですか？

エラー1：

```JSON
The server encountered an internal error and was unable to complete your request。Either the server is overloaded or there is an error in the application
```

エラー2：

```JSON
Rate limit reached for default-gpt-3.5-turboin organization org-wDrZCxxxxxxxxxissoZb on requestsper min。 Limit: 3 / min. Please try again in 20s. Contact us through our help center   at help.openai.com   if you continue to haveissues. Please add a payment method toyour account to increase your rate limit.Visit https://platform.openai.com/account/billingto add a payment method.
```

公式のAPI呼び出し速度制限に達している可能性があります。詳細は[OpenAI公式ドキュメント](https://platform.openai.com/docs/guides/rate-limits)を参照してください。

### 8. ユーザーが自己デプロイ後にスマートチャットを使用できず、以下のエラーが表示される場合：**Unrecognized request argument supplied: functions**、どのように解決すればよいですか？

まず、フロントエンドとバックエンドのバージョンが最新であり、かつ両者のバージョンが一致していることを確認してください。このエラーが発生する理由として、Azure OpenAIのキーを使用しているにもかかわらず、モデルが正常にデプロイされていない可能性があります。そのため、使用中のAzure OpenAIにモデルがデプロイされているかを確認してください。また、gpt-3.5-turboモデルのバージョンは0613以上である必要があります。（0613以前のバージョンはスマートチャットで使用されるfunction call機能をサポートしていないため、利用できません）

### 9. OpenAI Keyを設定する際に以下のエラーが表示される場合、原因は何ですか？

```JSON
Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError( <urllib3.connection.HTTPSConnection object at 0x7f0462ed7af0>; Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
```

このエラーは通常、環境にプロキシ設定があるために発生しますので、プロキシの設定を確認してください。

### 10. アプリ内でモデルを切り替える際に以下のエラーが発生した場合、どうすればよいですか？

```JSON
Anthropic: Error code: 400 - f'error': f'type': "invalid request error, 'message': 'temperature: range: -1 or 0..1)
```

各モデルのパラメータ値の範囲が異なるため、現在のモデルのパラメータ値の範囲に従って設定する必要があります。

### 11. 以下のエラーメッセージに遭遇した場合、どうすればよいですか？

```JSON
Query or prefix prompt is too long, you can reduce the prefix prompt, or shrink the max token, or switch to a llm with a larger token limit size
```

編成ページのパラメータ設定で、「最大トークン」の値を小さくすることで解決できます。

### 12. Difyにはデフォルトでどのモデルが使用されており、オープンソースのモデルを使用できますか？

デフォルトのモデルは **設定 - モデル供給業者** で構成でき、現在はOpenAI、Azure OpenAI、Anthropicなどのモデルプロバイダーのテキスト生成モデルがサポートされています。また、Hugging Face、Replicate、xinferenceがホストしているオープンソースモデルの統合もサポートされています。

### 13. ナレッジベースを **Q\&A 分割モード** に設定した際、常に待機中の表示が出るのはなぜですか？

使用しているEmbeddingモデルのAPIキーがレート制限に達している可能性があります。

### 14. ユーザーがアプリを使用中に「Invalid token」というエラーが発生した場合、どのように対処すればよいですか？

「Invalid token」というエラーが発生した場合、以下の2つの解決策を試してみてください：

* ブラウザのキャッシュ（クッキー、セッションストレージ、ローカルストレージ）をクリアし、スマートフォンを使用している場合は、アプリのキャッシュもクリアしてから再度アクセスしてください。
* 新しいアプリのURLを生成し、その新しいURLにアクセスしてください。

### 15. データセットのドキュメントアップロードにはどのようなサイズ制限がありますか？

現在、データセットのドキュメントアップロードでは、単一のドキュメントの最大サイズは15MB、総ドキュメント数の制限は100です。ローカルデプロイメントバージョンでこの制限を調整する必要がある場合は、[ドキュメント](/ja-jp/user-guide/faq/llm-using)を参照してください。

### 16. Claudeモデルを選択してもOpenAIの料金がかかるのはなぜですか？

ClaudeはEmbeddingモデルをサポートしていないため、Embeddingプロセスや他の対話生成はデフォルトでOpenAIのキーを使用します。これにより、引き続きOpenAIのクオータを消費します。設定のモデルプロバイダーで他のデフォルト推論モデルやEmbeddingモデルを設定することも可能です。

### 17. モデル自体の生成能力よりも、より多くのコンテキストデータを効果的に制御する方法はありますか？

データセットの利用の有無は、データセットの説明に依存します。データセットの説明をできるだけ明確に記述することが重要です。

### 18. データセットドキュメントのアップロードがExcel形式の場合、どのように適切にセグメント化できますか？

最初の行をヘッダーとして使用し、以降の各行には内容を表示します。余分なヘッダー設定や複雑な形式の表の内容は設定しないでください。

以下の例では、2行目のヘッダーのみを残し、最初の行（表1）は不要なヘッダーとして削除してください。

<Frame caption="">
  <img src="/ja-jp/img/en-faq-remove-header.png" alt="" />
</Frame>

### 19. ChatGPT Plusを購入したが、difyでまだGPT-4を使用できない理由は？

OpenAIのGPT-4モデルAPIとChatGPT Plusは異なる製品です。モデルのAPIには独自の価格設定があり、詳細は[OpenAI Pricing Document](https://openai.com/pricing)をご参照ください。支払いを申請するには、まずカードを登録する必要があります。カードを登録すると、GPT-3.5の権限が付与されますが、GPT-4の権限はありません。詳細については[OpenAI公式ドキュメント](https://platform.openai.com/account/billing/overview)をご覧ください。

### 20. 他のEmbedding Modelを追加するにはどうすればよいですか？

Difyは以下のEmbeddingモデルをサポートしています。設定ボックスで`Embeddings`タイプを選択するだけで追加できます。

* Azure
* LocalAI
* MiniMax
* OpenAI
* Replicate
* XInference

### 21. 自分で作成したアプリをアプリテンプレートとして設定する方法は？

この機能はDifyの公式が提供するアプリテンプレートをクラウド版ユーザーが参照するためのものであり、自分で作成したアプリをアプリテンプレートとして設定する機能は現在サポートされていません。クラウド版を使用している場合は、**ワークスペースに追加**または**カスタマイズ**を行い、自分自身のアプリに変更してください。

