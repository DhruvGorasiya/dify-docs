---
title: "LLM"
---

The `LLM` node is where the magic happens - it takes natural language, files, and images from workflow variables and transforms them into meaningful outputs. Whether you need text generation, analysis, or decision-making, the LLM node handles the heavy lifting.

![LLM Node](https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/85730fbfa1d441d12d969b89adf2670e.png)

## Configuration

Setting up an LLM node involves four main steps:

![LLM Node Configuration - Model Selection](https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/43f81418ea70d4d79e3705505e777b1b.png)

### **1. Choose Your Model**

<Info>
  New to Dify? You'll need to configure your model providers in **System Settings â†’ Model Providers** before you can select models in your LLM nodes.
</Info>

See Model Configuration [Guide](/en/guides/model-configuration/readme) for details.

### **2. Set Model Parameters**

These parameters control how your model generates responses:

**Temperature** (0-1): Controls creativity. Lower values (near 0) produce consistent, predictable responses. Higher values (near 1) increase randomness and creativity.

**Top P**: Limits word choices based on probability. Lower values focus on more likely words; higher values allow more diverse vocabulary.

**Presence Penalty**: Reduces repetition by penalizing topics the model has already mentioned. Higher values encourage discussing new topics.

**Frequency Penalty**: Penalizes frequently used words and phrases to increase vocabulary diversity. Higher values reduce word repetition.

Rather than tweaking individual settings, you can use our three presets: `Creative` (more inventive), `Balanced` (middle ground), or `Precise` (focused and consistent).

### **3. Write Your Prompts**

This is where you tell the model exactly what to do. The interface adapts to your chosen model type - chat models show System/User/Assistant sections, while completion models offer a simpler structure.

<Note>
  Some model providers may specify additional input rules - be sure to check your specific provider's API documentation.
</Note>

<Tip>
  Different models expect different role names (Human/Assistant vs User/AI). Adjust these settings to match your model's training may improve your output.
</Tip>

### **4. Configure Advanced Features**

**Memory**: Enable this to include conversation history in each LLM call. Essential for multi-turn dialogues where context matters. Memory is node-specific and not shared across workflow.

**Error Handling**: Set up backup paths / automatic retries when LLM call fails. See the [Error Handling guide](/en/guides/workflow/error-handling) for details.

**Structured Outputs**: The **JSON Schema Editor** in LLM nodes lets you force your LLM to return data in structured formats (JSON, lists, etc.) that you specify. You can use either the **Visual Editor** for a user-friendly experience or the **JSON Schema** for more precise control.

<Accordion title="JSON Schema Editor">
  Access the editor through **LLM Node \> Output Variables \> Structured \> Configure**. You can switch between visual and JSON Schema editing modes.

  <Info>
    JSON Schema Editor supports structured outputs across all models:

    - Models with Native Support: Can directly use JSON Schema definitions.
    - Models without Native Support: Not all models handle structured outputs reliably. We will include your schema in the prompt, but response formatting may vary by model.
  </Info>
  **_Visual Editor_**

  **When to Use**

  - For simple fields such as `name`, `email`, `age` without nested structures
  - If you prefer a drag-and-drop way over writing JSON
  - When you need to quickly iterate on your schema structure

  **Add Fields**

  Click **Add Field** and set parameters below:

  - _(required)_ Field Name
  - _(required)_ Field Type: Choose from string, number, object, array, etc.

    > Note: Object and array type fields can contain child fields.
  - Description: Helps the LLM understand what the field means.
  - Required: Ensures the LLM always includes this field in its output.
  - Enum: Restricts possible values. For example, to allow only red, green, blue:

  ```json
  {
    "type": "string",
    "enum": ["red", "green", "blue"]
  }
  ```

  **Manage Fields**

  - To Edit: Hover over a field and click the Edit icon.
  - To Delete: Hover over a field and click the Delete icon.

    > Note: Deleting an object or array removes all its child fields.

  **Import from JSON**

  1. Click **Import from JSON** and paste your example:

  ```json
  {
   "comment": "This is great!",
   "rating": 5
  }
  ```

  2. Click **Submit** to convert it into a schema.

  **Generate with AI**

  1. Click the AI Generate icon, select a model (like GPT-4o), and describe what you need:

  > "I need a JSON Schema for user profiles with username (string), age (number), and interests (array)."

  2. Click **Generate** to create a schema:

  ```json
  {
    "type": "object",
    "properties": {
      "username": {
        "type": "string"
      },
      "age": {
        "type": "number"
      },
      "interests": {
        "type": "array",
        "items": {
          "type": "string"
        }
      }
    },
    "required": ["username", "age", "interests"]
  }
  ```

  **_JSON Schema_**

  **When to Use**

  - For complex fields that need nesting, (e.g., `order_details`, `product_lists`)
  - When you want to import and modify existing JSON Schemas or API examples
  - When you need advanced schema features, such as `pattern` (regex matching) or `oneOf` (multiple type support)
  - When you want to fine-tune an AI-generated schema to fit your exact requirements

  **Add Fields**

  1. Click **Import from JSON** and add your field structure:

  ```json
  {
    "name": "username",
    "type": "string",
    "description": "user's name",
    "required": true
  }
  ```

  2. Click **Save**. Your schema will be validated automatically.

  **Manage Fields**: Edit field types, descriptions, default values, etc. in the JSON code box, and then click **Save**.

  **Import from JSON**

  1. Click **Import from JSON** and paste your example:

  ```json
  {
   "comment": "This is great!",
   "rating": 5
  }
  ```

  2. Click **Submit** to convert it into a schema.

  **Generate with AI**

  1. Click the AI Generate icon, select a model (like GPT-4o), and describe what you need:

  > "I need a JSON Schema for user profiles with username (string), age (number), and interests (array)."

  2. Click **Generate** to create a schema:

  ```json
  {
    "type": "object",
    "properties": {
      "username": {
        "type": "string"
      },
      "age": {
        "type": "number"
      },
      "interests": {
        "type": "array",
        "items": {
          "type": "string"
        }
      }
    },
    "required": ["username", "age", "interests"]
  }
  ```
</Accordion>

See this guide for a step-by-step walkthrough:

<iframe width="560" height="315" src="https://www.youtube.com/embed/u8NQFxrsaW0" title="Customer Information Intake Form Demo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen />

## Special Variables

Some variables have special powers in LLM nodes, unlocking specific features and capabilities:

### Context Variables

Context variables let you inject external content (like knowledge base results) into your prompts. They're essential for building RAG applications where you need to combine retrieved information with user queries.

![Context Variables](https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/5aefed96962bd994f8f05bac96b11e22.png)

Here's how it works: Your knowledge retrieval node outputs a `result` variable. Connect this to your LLM node's context variable, then insert that context variable into your prompt. Now your LLM has access to relevant knowledge from your database.

Context variables also enable [citation and attribution](/en/guides/knowledge-base/retrieval-test-and-citation#id-2-citation-and-attribution) - they track where information comes from so users can see sources.

<Info>
  You can use regular variables as context too, but you'll lose the citation feature since they don't contain source information.
</Info>

### File Variables

Modern LLMs like [Claude 3.5 Sonnet](https://docs.anthropic.com/en/docs/build-with-claude/pdf-support) can process files directly. Use file variables to feed documents, images, or other content straight into your prompts. Always check your model's documentation for supported file types first.

![](https://assets-docs.dify.ai/2024/11/05b3d4a78038bc7afbb157078e3b2b26.png)

Check out our [File Upload guide](/en/guides/workflow/file-upload) for help building file upload workflows.

### Conversation History

For text completion models that need conversational context, you can insert conversation history variables. This helps models understand the flow of multi-turn conversations by including previous exchanges in the prompt.

<Info>
  This is mainly for text completion models in Chatflow applications. Most chat models handle conversation history automatically.
</Info>

![Inserting Conversation History Variable](https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/b8642f8c6e3f562fceeefae83628fd68.png)

{/*
Contributing Section
DO NOT edit this section!
It will be automatically generated by the script.
*/}

---

[Edit this page](https://github.com/langgenius/dify-docs/edit/main/en/guides/workflow/node/llm.mdx) | [Report an issue](https://github.com/langgenius/dify-docs/issues/new?title=Documentation%20Issue%3A%20&body=%23%23%20Issue%20Description%0A%3C%21--%20Please%20briefly%20describe%20the%20issue%20you%20found%20--%3E%0A%0A%23%23%20Page%20Link%0Ahttps%3A%2F%2Fgithub.com%2Flanggenius%2Fdify-docs%2Fblob%2Fmain%2Fen/guides/workflow/node%2Fllm.mdx%0A%0A%23%23%20Suggested%20Changes%0A%3C%21--%20If%20you%20have%20specific%20suggestions%20for%20changes%2C%20please%20describe%20them%20here%20--%3E%0A%0A%3C%21--%20Thank%20you%20for%20helping%20improve%20our%20documentation%21%20--%3E)