---
title: LLM
---

The LLM node is where the magic happens. It processes user input from your Start node and generates intelligent responses using large language models.

This is your workflow's brain - it takes natural language, files, or images from users and transforms them into meaningful outputs. Whether you need text generation, analysis, or decision-making, the LLM node handles the heavy lifting.

![LLM Node](https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/85730fbfa1d441d12d969b89adf2670e.png)

## What You Can Build

The LLM node adapts to virtually any use case that requires language understanding or generation:

**Intent Recognition**: Automatically classify customer inquiries to route them to the right department or response.

**Content Creation**: Generate blog posts, product descriptions, or marketing copy based on your specifications.

**Email Processing**: Sort and categorize incoming emails as inquiries, complaints, or spam.

**Translation**: Convert text between languages while maintaining context and tone.

**Code Generation**: Write specific functions, APIs, or test cases based on requirements.

**Knowledge Synthesis**: Combine information from your knowledge base to answer complex questions.

**Image Analysis**: Use vision-capable models to understand and describe visual content.

**Document Processing**: Extract insights and summaries from uploaded files.

The key is choosing the right model and crafting effective prompts for your specific needs.

## Configuration

Setting up an LLM node involves four main steps that determine how your model behaves and responds.

![LLM Node Configuration - Model Selection](https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/43f81418ea70d4d79e3705505e777b1b.png)

**1. Choose Your Model**

Dify supports the major language models: OpenAI's GPT series, Anthropic's Claude series, and Google's Gemini series. Your choice depends on what you need - inference capability, cost efficiency, response speed, and context window size all matter. Match your model to your task requirements.

**2. Set Model Parameters**

These parameters control how your model generates responses - temperature affects creativity, TopP influences diversity, max tokens sets length limits. Rather than tweaking individual settings, you can use our three presets: Creative (more inventive), Balanced (middle ground), or Precise (focused and consistent).

**3. Write Your Prompts**

This is where you tell the model exactly what to do. The interface adapts to your chosen model type - chat models show System/User/Assistant sections, while completion models offer a simpler structure.

**4. Configure Advanced Features**

Enable memory for multi-turn conversations, set memory windows to control context length, and use Jinja-2 templates for complex prompt logic when needed.

<Info>
New to Dify? You'll need to configure your model providers in **System Settings â†’ Model Providers** before you can select models in your LLM nodes.
</Info>

### Writing Effective Prompts

Your prompts determine everything - they're the instructions that guide your model's behavior. For chat models, you'll work with System (sets behavior), User (current input), and Assistant (example responses) sections.

**Need Help Getting Started?**

Use the Prompt Generator when you're stuck. Describe your business scenario and it'll create a starter prompt using AI. You can then refine it for your specific needs.

![](https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/bec10045f819316f80068c563cf14eb1.png)

**Adding Variables to Prompts**

Type `/` or `{` in the prompt editor to bring up the variable menu. This lets you insert data from upstream nodes or special variables directly into your prompts, making them dynamic and context-aware.

![Calling Out the Variable Insertion Menu](https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/d8ed0160a7fba0a14dd823ef97610cc4.png)

## Special Variables

Some variables have special powers in LLM nodes. These aren't just regular data - they unlock specific features and capabilities.

### Context Variables

Context variables let you inject external content (like knowledge base results) into your prompts. They're essential for building RAG applications where you need to combine retrieved information with user queries.

![Context Variables](https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/5aefed96962bd994f8f05bac96b11e22.png)

Here's how it works: Your knowledge retrieval node outputs a `result` variable. Connect this to your LLM node's context variable, then insert that context variable into your prompt. Now your LLM has access to relevant knowledge from your database.

Context variables also enable [citation and attribution](/en/guides/knowledge-base/retrieval-test-and-citation#id-2-citation-and-attribution) - they track where information comes from so users can see sources.

<Info>
You can use regular variables as context too, but you'll lose the citation feature since they don't contain source information.
</Info>

### File Variables

Modern LLMs like [Claude 3.5 Sonnet](https://docs.anthropic.com/en/docs/build-with-claude/pdf-support) can process files directly. Use file variables to feed documents, images, or other content straight into your prompts. Always check your model's documentation for supported file types first.

![](https://assets-docs.dify.ai/2024/11/05b3d4a78038bc7afbb157078e3b2b26.png)

Need help building file upload workflows? Check out our [File Upload guide](/en/guides/workflow/file-upload).

### Conversation History

For text completion models that need conversational context, you can insert conversation history variables. This helps models understand the flow of multi-turn conversations by including previous exchanges in the prompt.

<Info>
This is mainly for text completion models in Chatflow applications. Most chat models handle conversation history automatically.
</Info>

![Inserting Conversation History Variable](https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/b8642f8c6e3f562fceeefae83628fd68.png)

## Model Parameters

These settings fine-tune how your model generates responses. Each model offers different parameters, but here are the key ones you'll encounter:

![](https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/5eaaa3f8082b769544a02ff510b207d8.png)

**Temperature** (0-1): Controls creativity. Lower values (near 0) produce consistent, predictable responses. Higher values (near 1) increase randomness and creativity.

**Top P**: Limits word choices based on probability. Lower values focus on more likely words; higher values allow more diverse vocabulary.

**Presence Penalty**: Reduces repetition by penalizing topics the model has already mentioned. Higher values encourage discussing new topics.

**Frequency Penalty**: Penalizes frequently used words and phrases to increase vocabulary diversity. Higher values reduce word repetition.

Don't want to tweak individual parameters? Use our presets: Creative (more inventive), Balanced (middle ground), or Precise (focused and consistent).

<img src="https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/b913f9cdf1f9b03e791a49836bc770dd.png" width="400" />

## Advanced Features

**Memory**: Enable this to include conversation history in each LLM call. Essential for multi-turn dialogues where context matters.

**Memory Window**: Control how much conversation history gets included. Leave it automatic or set specific limits based on your needs.

**Role Name Settings**: Different models expect different role names (Human/Assistant vs User/AI). Adjust these settings to match your model's training.

**Jinja-2 Templates**: Use advanced template logic in your prompts for data transformation and conditional content. Check the [official documentation](https://jinja.palletsprojects.com/en/3.1.x/templates/) for syntax.

**Retry on Failure**: Automatically retry failed requests up to 10 times with configurable intervals (max 5000ms). Useful for handling temporary API issues.

![](https://assets-docs.dify.ai/2024/12/dfb43c1cbbf02cdd36f7d20973a5529b.png)

**Error Handling**: Set up backup paths when nodes fail. Your workflow can continue running even if the LLM encounters problems. See our [Error Handling guide](/en/guides/workflow/error-handling) for details.

**Structured Outputs**: Force your LLM to return data in specific formats (JSON, lists, etc.). Perfect when you need predictable, parseable responses.

<Accordion title="JSON Schema Editor">

The **JSON Schema Editor** in LLM nodes lets you define how you want your data structured. You can use either the **Visual Editor** for a user-friendly experience or the **JSON Schema** for more precise control.

<Info>
JSON Schema Editor supports structured outputs across all models:

- Models with Native Support: Can directly use JSON Schema definitions.

- Models without Native Support: Not all models handle structured outputs reliably. We will include your schema in the prompt, but response formatting may vary by model.
</Info>

**Get Started**

Access the editor through **LLM Node > Output Variables > Structured > Configure**. You can switch between visual and JSON Schema editing modes.

![JSON Schema Editor](https://assets-docs.dify.ai/2025/04/646805384efa3cd85869d23a4d9735ad.png)

***Visual Editor***

**When to Use**

- For simple fields such as `name`, `email`, `age` without nested structures

- If you prefer a drag-and-drop way over writing JSON

- When you need to quickly iterate on your schema structure

![Visual Editor](https://assets-docs.dify.ai/2025/04/a9d6a34a7903f81e4d57c7f1d8d0712b.png)

**Add Fields**

Click **Add Field** and set parameters below:

- *(required)* Field Name

- *(required)* Field Type: Choose from string, number, object, array, etc.

  > Note: Object and array type fields can contain child fields.

- Description: Helps the LLM understand what the field means.

- Required: Ensures the LLM always includes this field in its output.

- Enum: Restricts possible values. For example, to allow only red, green, blue:

```json
{
  "type": "string",
  "enum": ["red", "green", "blue"]
}
```

**Manage Fields**

- To Edit: Hover over a field and click the Edit icon.

- To Delete: Hover over a field and click the Delete icon.

  > Note: Deleting an object or array removes all its child fields.

**Import from JSON**

1. Click **Import from JSON** and paste your example:

```json
{
 "comment": "This is great!",
 "rating": 5
}
```

2. Click **Submit** to convert it into a schema.

**Generate with AI**

1. Click the AI Generate icon, select a model (like GPT-4o), and describe what you need:

  > "I need a JSON Schema for user profiles with username (string), age (number), and interests (array)."

2. Click **Generate** to create a schema:

```json
{
  "type": "object",
  "properties": {
    "username": {
      "type": "string"
    },
    "age": {
      "type": "number"
    },
    "interests": {
      "type": "array",
      "items": {
        "type": "string"
      }
    }
  },
  "required": ["username", "age", "interests"]
}
```

***JSON Schema***

**When to Use**

- For complex fields that need nesting, (e.g., `order_details`, `product_lists`)

- When you want to import and modify existing JSON Schemas or API examples

- When you need advanced schema features, such as `pattern` (regex matching) or `oneOf` (multiple type support)

- When you want to fine-tune an AI-generated schema to fit your exact requirements

![JSON Schema](https://assets-docs.dify.ai/2025/04/669af808dd9d0d8521a36e14db731cec.png)

**Add Fields**

1. Click **Import from JSON** and add your field structure:

```json
{
  "name": "username",
  "type": "string",
  "description": "user's name",
  "required": true
}
```

2. Click **Save**. Your schema will be validated automatically.

**Manage Fields**: Edit field types, descriptions, default values, etc. in the JSON code box, and then click **Save**.

**Import from JSON**

1. Click **Import from JSON** and paste your example:

```json
{
 "comment": "This is great!",
 "rating": 5
}
```

2. Click **Submit** to convert it into a schema.

**Generate with AI**

1. Click the AI Generate icon, select a model (like GPT-4o), and describe what you need:

  > "I need a JSON Schema for user profiles with username (string), age (number), and interests (array)."

2. Click **Generate** to create a schema:

```json
{
  "type": "object",
  "properties": {
    "username": {
      "type": "string"
    },
    "age": {
      "type": "number"
    },
    "interests": {
      "type": "array",
      "items": {
        "type": "string"
      }
    }
  },
  "required": ["username", "age", "interests"]
}
```

</Accordion>

***

#### Use Cases

* **Reading Knowledge Base Content**

To enable workflow applications to read "[Knowledge Base](../../knowledge-base/)" content, such as building an intelligent customer service application, please follow these steps:

1. Add a knowledge base retrieval node upstream of the LLM node;
2. Fill in the **output variable** `result` of the knowledge retrieval node into the **context variable** of the LLM node;
3. Insert the **context variable** into the application prompt to give the LLM the ability to read text within the knowledge base.

![](https://assets-docs.dify.ai/2025/04/f62253ae895d9737399c50e93385534f.png)

The `result` variable output by the Knowledge Retrieval Node also includes segmented reference information. You can view the source of information through the **Citation and Attribution** feature.

<Info>
Regular variables from upstream nodes can also be filled into context variables, such as string-type variables from the start node, but the **Citation and Attribution** feature will be ineffective.
</Info>

* **Reading Document Files**

To enable workflow applications to read document contents, such as building a ChatPDF application, you can follow these steps:

* Add a file variable in the "Start" node;
* Add a document extractor node upstream of the LLM node, using the file variable as an input variable;
* Fill in the **output variable** `text` of the document extractor node into the prompt of the LLM node.

For more information, please refer to [File Upload](../file-upload).

![](https://assets-docs.dify.ai/2025/04/c74cf0c58aaf1f35e515044deec2a88c.png)

* **Error Handling**

When processing information, LLM nodes may encounter errors such as input text exceeding token limits or missing key parameters. Developers can follow these steps to configure exception branches, enabling contingency plans when node errors occur to avoid interrupting the entire flow:

1. Enable "Error Handling" in the LLM node
2. Select and configure an error handling strategy

![Error handling](https://assets-docs.dify.ai/2024/12/f7109ce5e87c0e0a81248bb2672c7667.png)

For more information about exception handling methods, please refer to the [Error Handling](https://docs.dify.ai/guides/workflow/error-handling).

* **Structured Outputs**

**Case: Customer Information Intake Form**

Watch the following video to learn how to use JSON Schema Editor to collect customer information:

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/u8NQFxrsaW0"
  title="Customer Information Intake Form Demo"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowfullscreen
></iframe>

{/*
Contributing Section
DO NOT edit this section!
It will be automatically generated by the script.
*/}

---

[Edit this page](https://github.com/langgenius/dify-docs/edit/main/en/guides/workflow/node/llm.mdx) | [Report an issue](https://github.com/langgenius/dify-docs/issues/new?title=Documentation%20Issue%3A%20&body=%23%23%20Issue%20Description%0A%3C%21--%20Please%20briefly%20describe%20the%20issue%20you%20found%20--%3E%0A%0A%23%23%20Page%20Link%0Ahttps%3A%2F%2Fgithub.com%2Flanggenius%2Fdify-docs%2Fblob%2Fmain%2Fen/guides/workflow/node%2Fllm.mdx%0A%0A%23%23%20Suggested%20Changes%0A%3C%21--%20If%20you%20have%20specific%20suggestions%20for%20changes%2C%20please%20describe%20them%20here%20--%3E%0A%0A%3C%21--%20Thank%20you%20for%20helping%20improve%20our%20documentation%21%20--%3E)

